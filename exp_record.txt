Combine output and hidden. 2 RNN
84 103.0 0.8155339805825242


Only hidden. 2 RNN
82 103.0 0.7961165048543689


Transformer:
79 103.0 0.7669902912621359

Transformer init state=0:
77 103.0 0.7475728155339806

Transformer init state=0, decoder T=16:
83 103.0 0.8058252427184466

Transformer init state=0, decoder T=16, decoder_layer=2, encoder_layer=2
80 103.0 0.7766990291262136

Transformer init state=0, decoder T=64, decoder_layer=2, encoder_layer=2
86 103.0 0.8349514563106796

With mask?

Learning rate=0.03
Transformer init state=0, decoder T=64, decoder_layer=2, encoder_layer=2
90 103.0 0.8737864077669902


Adam, no LR anneal
Learning rate=0.01
Transformer init state=0, decoder T=64, decoder_layer=2, encoder_layer=2
Failed to converge.


Learning rate=0.1
Transformer init state=0, decoder T=64, decoder_layer=2, encoder_layer=2, Dropout=0.2
95 103.0 0.9223300970873787


Learning rate=0.1
Transformer init state=0, decoder T=64, decoder_layer=2, encoder_layer=2, Dropout=0.1
92 103.0 0.8932038834951457


Learning rate=0.1
Transformer init state=0, decoder T=64, decoder_layer=2, encoder_layer=2, Dropout=0.5
96 103.0 0.9320388349514563


Learning rate=0.1
Transformer init state=0, decoder T=64, decoder_layer=2, encoder_layer=2, Dropout=0.8
Failed to converge.


Learning rate=0.1
Transformer init state=0, decoder T=64, decoder_layer=2, encoder_layer=2, Dropout=0.6
95 103.0 0.9223300970873787


TODO:
transform mask.




DeepSpeech(
  (conv): MaskConv(
    (seq_module): Sequential(
      (0): Conv2d(1, 32, kernel_size=(41, 11), stride=(2, 2), padding=(20, 5))
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Hardtanh(min_val=0, max_val=20, inplace=True)
      (3): Conv2d(32, 32, kernel_size=(21, 11), stride=(2, 1), padding=(10, 5))
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): Hardtanh(min_val=0, max_val=20, inplace=True)
    )
  )
  (fc): Sequential(
    (0): BatchNorm1d(83968, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): Linear(in_features=83968, out_features=2048, bias=True)
    (2): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (3): ReLU()
    (4): Linear(in_features=2048, out_features=1024, bias=True)
    (5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Linear(in_features=1024, out_features=3, bias=False)
  )
  (inference_softmax): InferenceBatchSoftmax()
  (transformer): Transformer(
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=1312, out_features=1312, bias=True)
          )
          (linear1): Linear(in_features=1312, out_features=2048, bias=True)
          (dropout): Dropout(p=0.5, inplace=False)
          (linear2): Linear(in_features=2048, out_features=1312, bias=True)
          (norm1): LayerNorm((1312,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1312,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.5, inplace=False)
          (dropout2): Dropout(p=0.5, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=1312, out_features=1312, bias=True)
          )
          (linear1): Linear(in_features=1312, out_features=2048, bias=True)
          (dropout): Dropout(p=0.5, inplace=False)
          (linear2): Linear(in_features=2048, out_features=1312, bias=True)
          (norm1): LayerNorm((1312,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1312,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.5, inplace=False)
          (dropout2): Dropout(p=0.5, inplace=False)
        )
      )
      (norm): LayerNorm((1312,), eps=1e-05, elementwise_affine=True)
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=1312, out_features=1312, bias=True)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): Linear(in_features=1312, out_features=1312, bias=True)
          )
          (linear1): Linear(in_features=1312, out_features=2048, bias=True)
          (dropout): Dropout(p=0.5, inplace=False)
          (linear2): Linear(in_features=2048, out_features=1312, bias=True)
          (norm1): LayerNorm((1312,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1312,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((1312,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.5, inplace=False)
          (dropout2): Dropout(p=0.5, inplace=False)
          (dropout3): Dropout(p=0.5, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=1312, out_features=1312, bias=True)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): Linear(in_features=1312, out_features=1312, bias=True)
          )
          (linear1): Linear(in_features=1312, out_features=2048, bias=True)
          (dropout): Dropout(p=0.5, inplace=False)
          (linear2): Linear(in_features=2048, out_features=1312, bias=True)
          (norm1): LayerNorm((1312,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((1312,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((1312,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.5, inplace=False)
          (dropout2): Dropout(p=0.5, inplace=False)
          (dropout3): Dropout(p=0.5, inplace=False)
        )
      )
      (norm): LayerNorm((1312,), eps=1e-05, elementwise_affine=True)
    )
  )
)